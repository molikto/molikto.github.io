<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>statistics notes</title>
        <link rel="stylesheet" type="text/css" href="../css/snailya.css" />
    </head>
    <body>
    <script src="../js/jquery.min.js"></script>

    <div class="navbar navbar-default" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../">snailya</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="../mathematics.html">mathematics</a></li>
            <li><a href="../random-notes.html">random notes</a></li>
            <li><a href="../posts.html">posts</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="https://github.com/molikto">github</a></li>
            <li><a href="http://stackexchange.com/users/1154520/molikto?tab=accounts">stackexchange</a></li>
          </ul>
        </div>
      </div>
    </div>
  <div class="body small-container" id="content">
    



<script>
$("#content").css("max-width", "1100px")
</script>


<div class="row">
    <div class="bs-docs-container">
        <div class="col-md-3">
            <div class="bs-docs-sidebar hidden-print" role="complementary">
            </div>
        </div>
    </div>
    <div class="col-md-8">
        <h1 id="statistics">statistics</h1>
<h2 id="probability">probability</h2>
<h3 id="formal-definition-in-measure-theory">formal definition in measure theory</h3>
<p><em>i have not read any books about this, but you can get much insight from wikipedia articles</em></p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Random_variable#Measure-theoretic_definition">http://en.wikipedia.org/wiki/Random_variable#Measure-theoretic_definition</a></li>
<li>probability space - measured space with total measure 1</li>
<li>random variable - function from <strong>measured space</strong> to <strong>measurable space</strong>, thus the latter measurable space can get an measure from the preimage
<ul>
<li>so what is a cdf? it is the measure function, so actually <code>R</code> can have various measures</li>
<li>so pdf is not in favour, because it is not a good measure, it only coresponds to <strong>differentiable measure</strong></li>
</ul></li>
<li>conditional - induced normalized measure for subspace</li>
<li><strong>fix me</strong> the conditional and join random variables is not well understood in the perspective now…</li>
</ul>
<h3 id="probability-1">probability</h3>
<ul>
<li>sample space, point: outcome, event, subset: events</li>
<li>probability
<ul>
<li>sigma-algebra, measurable space</li>
<li>non-neg</li>
<li>unity</li>
<li>sigma-additivity</li>
</ul></li>
<li>what it means?
<ul>
<li>freq</li>
<li>Bayesian</li>
<li>in my opinion it do not matters… the model is there, it is just a matter of calculation</li>
<li>but it do lead to different methods</li>
</ul></li>
<li><code>A_n -&gt; A =&gt; P(A_n) -&gt; P(A)</code>: using def, and measure, and properties of limits
<ul>
<li>review: this is the proposition 1 in the positive measure section</li>
</ul></li>
<li><code>P(A|B) = P(AB) / P(B)</code>
<ul>
<li>is this kind of submeasure?</li>
</ul></li>
<li>event independent <code>P(AB) = P(A)P(B)</code>
<ul>
<li><code>P(A|B) = P(A)</code></li>
</ul></li>
<li>Bayes’ theorem <code>P(B) = Sum( P(B|A_i)P(A_i) )</code> is just set ops + simple probobility
<ul>
<li><code>P(A_i|B) = P(B|A_i)P(A_i) / P(B)</code></li>
<li><code>P(A_i)</code> the prior probability of <code>A</code></li>
<li><code>P(A_i|B)</code> the posterior probability of <code>A</code></li>
</ul></li>
</ul>
<h3 id="random-variables">random variables</h3>
<p><em>they are functions on a measure space to R, and you get a measure on R</em></p>
<p><em>the so called multivarance is just linear algebra + rv</em></p>
<p><em>if you do not consider the distribution, then what random <strong>variable</strong> means is that it is normal variables, but you can add them, try to picture youself <code>X-bar/n</code></em></p>
<p><em>join dis is defined by <strong>P</strong>, consider youself the join dis of <code>X</code> and <code>1-X</code>? you cannot picture this! but it do has a cpf! it start at the diag from 0 to 1, but is is not differentable! (what is 2 differentable?)</em></p>
<ul>
<li>random variable <code>X</code>: space to <code>R</code></li>
<li><code>P(X in A) = P(X^-1(A))</code></li>
<li><code>P(X = x) = P(X^-1(x))</code></li>
<li>cumulative distribution function <code>F_X(x): R -&gt; [0, 1] = P(X &lt;= x)</code>
<ul>
<li>theorem 2.7: CDF determines the probability structure
<ul>
<li>review: the induced measure is same! not the rv! for exmaple. <code>f_1 = x</code> and <code>f_2 = -x</code> for a normal</li>
<li>so we must remember that, the function that induced the random variable is always essential</li>
</ul></li>
<li>theorem 2.8: <code>R -&gt; [0,1]</code> is cdf &lt;=&gt;
<ul>
<li>non-decreasing</li>
<li><code>F(-inf) = 0</code> and <code>F(inf) = 1</code></li>
<li><code>F(x) = lim_{y -&gt; x, y &gt; x} F(y)</code>
<ul>
<li>review: this actually concerned with that cdf is defined <code>P(X &lt;= x)</code>, it is a asymetry</li>
</ul></li>
</ul></li>
<li><code>P(X = x) = F(x) - F(x-)</code></li>
<li><code>P(x &lt; X &lt;= y) = F(y) - F(x)</code></li>
</ul></li>
<li>inverse cdf <code>F^-1(q) = inf(x: F(x) &gt; q)</code>
<ul>
<li>if <code>F</code> is strictly increasing and continuous, then it is just function inverse</li>
<li>a simple example: try to picture the inverse of a normal</li>
</ul></li>
<li>probability function <code>f_X(x) = P(X = x)</code></li>
<li>probability density functiion: <code>F_X(x) = integral from -inf to x f_X(t) dt</code>
<ul>
<li>if such a function exists, <code>X</code> is called continuous</li>
<li><code>F</code> is differentiable</li>
<li><code>P(X = x) = 0</code>, this is necessory, because you can using integral!!!</li>
<li>pdf can be unbounded</li>
<li>review: a pdf exists relays on the underline measure space to is differentiable, so it concerns about linearity, so it concerns if it is <code>R</code> or something</li>
</ul></li>
<li>join pf and join cpf
<ul>
<li>consider multivarance normal, so two join has far more different shapes, it is what it means to have a sample space, but when it is independent, we can see it is uniquely defined</li>
<li>independent always means factorable</li>
</ul></li>
<li>pdf of <code>(X, Y)</code></li>
<li>marginal probability function <code>f_X(x)</code>, <code>f_Y(y)</code></li>
<li>marginal cdf <code>F_X</code>, <code>F_Y</code></li>
<li>marginal density <code>f_X(x) = integral f(x, y) dy</code></li>
<li><code>X</code>, <code>Y</code> independent: <code>forall A, B: P(X in A, Y in B) = P(X in A)P(Y in B)</code>
<ul>
<li>for differentialbe &lt;=&gt; <code>f_X,Y = f_X f_Y</code></li>
<li><em>independence of random variable means we can factor the sample space, and reguard the two variable only dependent on one subspace!</em></li>
</ul></li>
<li>conditional pf: <code>f_{X|Y}(x|y) = P(X = x| Y = y)</code> if <code>P(Y = y) &gt; 0</code></li>
<li>conditinal pdf: <code>f_{X|Y}(x|y) = f_{X|Y}(x|y) / f_Y(y)</code>
<ul>
<li><code>P(X in A| Y = y) = integral_A f</code></li>
</ul></li>
<li>tranform random variable, or variables =&gt; variable
<ul>
<li>actually it is always about calculation of a pdf or cdf</li>
</ul></li>
<li>iid sample</li>
</ul>
<h3 id="functional">functional</h3>
<p><em>functionals from the function space to <code>R</code></em></p>
<ul>
<li>median <code>F^-1(1/2)</code></li>
<li><code>E(X) = integral x dF(x) = EX = mu_X</code>
<ul>
<li><em>if you have two variable, you need two integral. thinking the num of rv as the dimension!!! picture youself a multivariance nomral</em></li>
<li>well defined: <code>integral |x| dF(x) &lt; inf</code></li>
<li><code>E(r(X)) = integral r(x) dF(x)</code></li>
<li><code>E(Sum(a_i X_i)) = Sum(a_i E(X_i))</code>
<ul>
<li>dirrectly proofed from lineary of integral, but I need better real analysis</li>
</ul></li>
<li><code>X_i</code> independent =&gt; <code>E(Prod(X_i)) = Prod(X_i)</code>
<ul>
<li>think of integral when id, the integral will be independent</li>
</ul></li>
<li><code>E(a'X) = a'mu</code>, <code>E(AX) = A mu</code></li>
</ul></li>
<li>k-th moment <code>E(X^k)</code>
<ul>
<li>well defined for k</li>
<li>k-th moment exists =&gt; j &lt; k exists</li>
</ul></li>
<li>variance <code>sigma^2 = E(X-mu)^2 = V(X)</code>
<ul>
<li><em>think of a dis, and the mu is a y = mu thing, it will be a lot easy!</em></li>
<li>standard deviation <code>sd(X) = sqrt(V(X)) = sigma</code></li>
<li><code>V(X) = E(X^2) - mu^2</code></li>
<li><code>V(aX+b) = a^2V(X)</code>, obvious when you thinking the line</li>
<li><code>X_i</code> independent =&gt; <code>V(Sum(a_i X_i)) = Sum(a_i^2     V(X_i))</code>, use the same method in expectation</li>
</ul></li>
<li>skewness <code>k = E(X - mu)^3/sigma^3</code></li>
<li>covariance <code>Cov(X, Y) = E((X - mu_X)(Y - mu_Y))</code>
<ul>
<li>corelation <code>rho = Cov(X, Y) / sigma_X sigma_Y</code></li>
<li><code>Cov(X, Y) = E(XY) - E(X)E(Y)</code>
<ul>
<li><em>use the two var thing!!!</em></li>
</ul></li>
<li><code>-1 &lt; rho &lt; 1</code>, expand the integral</li>
<li>when <code>X</code> and <code>Y</code> is linear or independent</li>
<li><code>V(X + Y) = V(X) + V(Y) + 2Cov(X, Y)</code>
<ul>
<li><code>V(Sum(a_i X_i)) = Sum(a_i^2 V(X_1)) + 2 SumSum a_i a_j Cov(X_i, X_j)</code></li>
</ul></li>
<li>variance-covariance matrix <code>Pi</code></li>
<li><code>V(a'X) = a' Pi a</code>, <code>V(AX) = A Pi A'</code></li>
</ul></li>
<li>conditional exception <code>E(X|Y = y) = integral x f dx</code>, <code>E(r(X,Y)| Y = y) = integral r(x, y) f dx</code> is a random variable of <code>y</code>
<ul>
<li>law of total exception: <code>E(E(r(X,Y)| X)) = E(r(X,Y))</code></li>
<li><em>this is a funcional that F(R^2) -&gt; F(R)</em></li>
</ul></li>
<li>conditional variance <code>V(Y|X = x) = integral (y - mu(x))^2 f(y|x) dy</code>
<ul>
<li>law of total: <code>V(Y) = EV(Y|X) + VE(Y|X)</code>
<ul>
<li><em>proof?</em></li>
</ul></li>
</ul></li>
<li>moment generating functions, Laplace transform <code>chi_X(t) : R -&gt; R = E(e^(tX))</code>
<ul>
<li><strong>fix me</strong></li>
</ul></li>
<li><code>X_i</code> idd
<ul>
<li>sample mean <code>X_n-bar = Sum(X_n) / n</code></li>
<li>sample variance <code>S_n^2 = Sum((X_i - X_n-bar)^2) / (n - 1)</code></li>
<li><code>E(X_n-bar) = mu</code></li>
<li><code>V(X_n-bar) = sigma^2 / n</code></li>
<li><code>E(S^2) = sigma^2</code>
<ul>
<li>it means it is unbiased</li>
<li><em>really cmompute this thing! it is easy!</em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="useful-examples-of-rv">useful examples of rv</h3>
<ul>
<li>multivarance normal <strong>fix me</strong></li>
</ul>
<h3 id="inequalities">inequalities</h3>
<ul>
<li>Markov’s: <code>X</code>, <code>t</code> &gt; 0 =&gt; <code>P(X &gt; t) &lt;= E(X) / t</code></li>
<li>Chebyshev’s <code>P(|X-mu| &gt;= t) &lt;= sigma^2 / t^2</code> and <code>P(|(X-mu)/sigma| &gt;= k) &lt;= 1/k^2</code></li>
<li>Hoeffding’s <strong>fix me</strong></li>
<li>Mill’s</li>
<li>Cauchy-Schwartz <code>E|XY| &lt;= sqrt(E(X^2)E(Y^2))</code></li>
<li>Jensen’s <code>g</code> convex =&gt; <code>Eg(X) &gt;= g(EX)</code></li>
</ul>
<h3 id="convergence-of-rv">convergence of rv</h3>
<p><em>they are just normal convergence in functional analysis</em></p>
<p><a href="http://en.wikipedia.org/wiki/Convergence_of_random_variables">http://en.wikipedia.org/wiki/Convergence_of_random_variables</a></p>
<ul>
<li>p: converges in <strong>probability</strong> <code>forall e, P(|X_n - X| &gt; e) -&gt; 0 as n -&gt; 0</code>
<ul>
<li>add, mul, map</li>
</ul></li>
<li>d: converges in <strong>distribution</strong> <code>lim F_n(t) -&gt; F(t)</code>, pointwize
<ul>
<li>const add, const mul, map</li>
</ul></li>
<li>qm: converges in <strong>quadratic mean</strong> <code>E(X_n - X)^2 -&gt; 0</code>
<ul>
<li>add</li>
</ul></li>
<li>qm =&gt; p =&gt; d
<ul>
<li>special case: point mass d &lt;=&gt; p</li>
</ul></li>
<li>weak law of large numbers: <code>X_n-bar -P-&gt; mu</code></li>
<li>clt: <code>Z_n = (X_n-bar - mu)/sqrt(V(X_n-bar)) = sqrt(n)(X_n-bar - mu)/sigma -d-&gt; N(0, 1)</code>
<ul>
<li>variance -&gt; sample variance is ok</li>
<li>multivariate version <code>sqrt(n)(X-bar - mu) -d-&gt; N(0, Pi)</code></li>
<li>delta method: <code>sqrt(n)(Y_n - mu) / sigma -d-&gt; N(0, 1)</code> &amp;&amp; <code>g</code> differentiable and <code>g'(mu) != 0</code> =&gt; <code>g(Y_n) -d-&gt; N(g(mu)</code>, <code>g'(mu)^2 * sigma^2 / n)</code>
<ul>
<li>multivariate delta method <strong>fix me</strong></li>
</ul></li>
</ul></li>
</ul>
<h2 id="stochastic-process">stochastic process</h2>
<ul>
<li>stochastic process, state space, index set</li>
<li>Markov chain <code>f(x_1,...,x_n) = f(x_1)f(x_2|x_1)...</code>
<ul>
<li>qestions
<ul>
<li>when settle down</li>
<li>parameter estimate</li>
<li>how to construct to converge</li>
</ul></li>
</ul></li>
<li>homogeneous
<ul>
<li>transition probabilities, transition matrix</li>
<li><code>p_ij(n)</code> n-step transition probabilities
<ul>
<li><code>p_ij(m + n) = Sum( p_ik(m) + p_kj(n) )</code></li>
</ul></li>
<li>simulation, <code>mu_0</code> initial distribution
<ul>
<li><code>mu_n = m_0 P^n</code></li>
</ul></li>
<li>reaches, communicate (is a equvilent)</li>
<li>irreducible, closed states, absorbing state</li>
<li>recurrent = persistent, transient
<ul>
<li>recurrent &lt;=&gt; <code>Sum (p_ii (n)) = Inf</code></li>
<li>communicate preserve recurrent and transient</li>
<li>finite Markov chain must has one recurrent state, if it is irreducible, all state is recurrent (simple)</li>
</ul></li>
<li>decomposition theorem: state space <code>X = X_T \/ X_i</code>, <code>X_T</code> is trans, <code>X_i</code> is irreducible recurrent (just use partition)</li>
<li>recurrent time (a rv): <code>T_ij = min{n | X_n = j}</code>
<ul>
<li>mean recurrent time <code>m_i = E(T_ii) = Sum(n f_ii(n))</code></li>
</ul></li>
<li>null, positive recurrent state
<ul>
<li>null -&gt; <code>p_ii(n) -&gt; 0</code></li>
<li>finite state -&gt; all positive</li>
</ul></li>
<li>period <code>d = gcd{n|p_ii(n) &gt; 0}</code>
<ul>
<li>periodic: <code>d &gt; 1</code></li>
<li>aperiodic: <code>d = 1</code></li>
</ul></li>
<li>ergodic = recurrent &amp; positive &amp; aperiodic (state | chain)
<ul>
<li>e.g. <em>23.28 Example</em></li>
</ul></li>
<li>stationary = invariant
<ul>
<li>stationary not necessory to converge!!!</li>
</ul></li>
<li>limitint distribution <code>P^n -&gt; [pi; pi; pi; ...]</code>, here <code>pi</code> is a vector!!!</li>
<li>irreducible, ergogic Markov chain has unique stationary distribution <code>pi</code>, limiting distribution is also <code>pi</code>, <code>g</code> bounded =&gt; <code>lim_N Sum(g(X_n))/N -&gt; E_pi(g) = Sum(g(j) pi)</code></li>
<li>detailed balance <code>pi_i p_ij = p_ji pi_j</code>
<ul>
<li>detailed balance =&gt; stationary distribution</li>
</ul></li>
<li><strong>fix me</strong> 23.31</li>
</ul></li>
<li>Possion processes <strong>fix me</strong></li>
</ul>
<h2 id="statistical-inference">statistical inference</h2>
<p><em>statistical inference is not solving equtions, because you have noise, you cannot get exact input/output, and so nomrally your knowns is not the degree of freedom of the system!!! and you do not solve exactly!!! they are not same problem at all!!!</em></p>
<h3 id="statistical-inference-1">statistical inference</h3>
<p><em>theory of inference</em></p>
<ul>
<li>kinds of inference model: given <code>X_i</code> the sample rv
<ul>
<li>non-parametric model
<ul>
<li><code>F</code></li>
<li><code>E(X)</code> - this is of course simpler, because we applied a very concentrating function</li>
<li>…</li>
</ul></li>
<li>parametric model
<ul>
<li><code>paramemters</code> - same as <code>F</code>, because we have a smaller search space for <code>F</code> for we have <strong>constrains</strong>, so we have a function to map parameters to <code>F</code>s</li>
<li>sometimes we estimate functional of estimators…, mostly <code>E</code> and <code>V</code></li>
</ul></li>
</ul></li>
<li>statistical model, parametric model, parameter space, nuisance parameters</li>
<li>predictor = regressor = feature = independent variable, outcome = response = dependent v</li>
<li>parameter regression model, nonparametric regression model (infinite-dim regression), prediction, classification, regression = curve estimation</li>
<li>freq inference, Bayesian inference</li>
<li>point estimation <code>theta_n-head = g(X_1,...,X_n)</code> for some <code>g</code> is a random variable
<ul>
<li><code>bias(theta_n-head) = E_theta(theta_n-head) - theta</code>, unbiased
<ul>
<li>this is a scalar, it is a statistical functional applied to a rv, so a saclar!!!</li>
</ul></li>
<li>consistent <code>-p-&gt;</code></li>
<li>sample distribution</li>
<li>standard error <code>se = sqrt(V(theta_n-head))</code> is applied value! again a salar!</li>
<li>ese <code>se-head</code></li>
<li>6.8 - see a worked out example for Bernoulli!
<ul>
<li>when calculating <code>V(p^n)</code>, use <code>X^2</code> and independence expansion!</li>
</ul></li>
<li><code>MSE = E(theta^_n - theta)^2 = bias^2 + V_theta</code>
<ul>
<li>this is not the variance of <code>theta^_n</code>! this is only true when you are unbias!!!</li>
</ul></li>
<li>e.g
<ul>
<li>parameter</li>
<li>cdf</li>
<li>pdf</li>
</ul></li>
<li><code>bias -&gt; 0 &amp;&amp; se -&gt; 0 =&gt; MSE -&gt; 0 =&gt; -pm-&gt; =&gt; -p-&gt;</code></li>
<li>asymptotically normal <code>(theta_n-head - theta) / se -&gt; N(0, 1)</code></li>
</ul></li>
<li><code>1-a</code> confidence interval <code>P(theta in C_n) &gt;= 1-a</code>, <code>C_n</code> is<code>(a(X_1,...,X_n), b(...))</code> is a <strong>random variable</strong>!!
<ul>
<li>confidence set: when multivariance</li>
<li>normal-based confidence interval <em>p94</em>
<ul>
<li>comparing 6.17 and 6.15, notice that normal-based only has large approximately correct coverage</li>
</ul></li>
</ul></li>
<li>hypothesis testing</li>
</ul>
<h3 id="nonparameter-mentods-for-cdf-and-functionals">nonparameter mentods for cdf and functionals</h3>
<p><em>I think we have enough error terms to do this! namely the convergence and bias!</em></p>
<ul>
<li>empirical distribution function <code>F_n-head(x)</code> it is <code>R -&gt; F(R)</code>, given a <code>x</code> it has a estimation of the value at this <code>x</code>
<ul>
<li>at <code>x</code>
<ul>
<li><code>E(F-head) = F(x)</code>, is unbiased</li>
<li><code>MSE = V(F-head) = F(x)(1-F(x)) / n</code></li>
<li><code>F_n-head(x) -P-&gt; F(x)</code>, it is consistant</li>
</ul></li>
<li><code>sup_x |F_n-head(x) - F(x)| -P-&gt; 0</code>
<ul>
<li><code>F^_n</code> give the function space a measure, and sup translate this measure again into scalar, and it is porobility limit to 0
<ul>
<li>DKW inequality <strong>fix me</strong></li>
</ul></li>
<li>confidence interval based on DKW</li>
</ul></li>
</ul></li>
<li>plug-in estimator of statistical functional <code>T(F)</code>: <code>theta_n-head = T(F_n-head)</code>, a functional is takes a rv to value, then you plugin to get a rv again!!!
<ul>
<li>lienar functional <code>T(F) = integral r(x) d F(x)</code>, <code>T</code> is linear in arguments!</li>
<li>plug-in estimator for linear funtional <code>T(F_n-head) = Sum(r(X_i)) / n</code> is a rv!</li>
<li>and we want <code>se</code> for this rv…</li>
<li>in many cases… <code>T(F_n-head) ~ N(T(F), se-head^2)</code>, means that we have a good enough estimator! and how to understand right??? you should divide to the left youself!!!</li>
<li>then <code>1-a</code> ci is <code>T(F_n-head) +- z_{a/2} se-head</code></li>
<li>e.g. it is just crazy… everything is rv!
<ul>
<li>mean - estimator, se-estimator, confidence interval</li>
<li>variance - plugin, sample variance <code>S^2_n</code></li>
<li>skewness</li>
<li>correlation - sample corelation</li>
<li><em>7.15</em></li>
</ul></li>
<li>in paramter model, we has formula for errors, but in nonparamter model, we mostly use bootstrap method</li>
</ul></li>
</ul>
<h3 id="bootstrap-method">bootstrap method</h3>
<p><em>in essence, bootstrap is just like plugin methods</em></p>
<ul>
<li>step
<ol style="list-style-type: decimal">
<li>estimate <code>V_F(T_n)</code> by <code>V_F^(T_n)</code>: if we can stop here, it is just plugin method</li>
<li>approximate <code>V_F^(T_n)</code> by simulation</li>
</ol></li>
<li>illu
<ul>
<li>real world <code>F</code> =&gt; <code>X_i</code> =&gt; <code>T_n</code></li>
<li>bootstrap world <code>F^_n</code> =&gt; <code>X*_i</code> =&gt; <code>T*_n</code></li>
</ul></li>
</ul>
<p><strong>fix me</strong></p>
<h3 id="parametric-inference">parametric inference</h3>
<p><em>if the world is a determined world, there is no noise at all, noise exists, is because our model is problamic or there is other things, but we think them as random</em></p>
<ul>
<li>the method of moments, assume <code>theta = (theta_1,..., theta_k)</code>
<ul>
<li>moment <code>a_j = a_j(theta) = integral x^j d F_theta(x)</code>, note that it is an function of theta, because we are having unknown theta</li>
<li>sample moment <code>a_j-head = Sum(X_i^j) / n</code></li>
<li>method: <code>forall j in 1 -&gt; k: a_(theta_n-head) = a_j-head</code>, it has <code>k</code> unknown of rvs, and we can solve for to get the estimator</li>
<li>properties
<ul>
<li>exits with probability tending 1</li>
<li>consistent</li>
<li>asympototically normal</li>
</ul></li>
<li><strong>fix me</strong> not well understood</li>
</ul></li>
<li>maximum likelihood method
<ul>
<li><em>most of the things bellow is from information theory!!!! so read them when have time!!!</em></li>
<li>likelihood function <code>L_n(theta) = Prod(f(X_i; theta))</code>, log-likelihood funtion <code>l_n(theta) = log(L_n(theta))</code>. give a param, get a rv</li>
<li>method: the thet a maximaze it! using derivative! you get an estimator!</li>
<li>multivar: we need global maxima, can use partial to find</li>
<li>properties (under certain regularity conditions of the model, rf <a href="http://en.wikipedia.org/wiki/Maximum_likelihood">http://en.wikipedia.org/wiki/Maximum_likelihood</a>)
<ul>
<li>consistent
<ul>
<li>Kullback-Leibler distance, information gain</li>
<li>first proof <code>M_n</code> converge to <code>-D(theta*, theta)</code>, if convergence is uniform over <code>theta</code>, then we can proof <code>theta^_n -p-&gt; theta*</code></li>
</ul></li>
<li>equivariant <code>g(theta)</code></li>
<li>asymptotically normal
<ul>
<li>score function <code>s(X;theta) = &amp; log f(X; theta) / &amp; theta</code></li>
<li>Fisher information <code>I_n(theta) = Sum( V_theta(s(X_i; theta)) )</code></li>
<li><code>I_n(theata) = nI(theta)</code></li>
<li><code>I(theta) = E_theta(-s')</code></li>
<li><code>se = sqrt(1/I_n(theta))</code></li>
<li><code>se^</code></li>
<li><code>(theta^_n - theta) /  se^ -d-&gt; N(0, 1)</code></li>
<li>asymptotic confidence interval</li>
</ul></li>
<li>asymptotically optimal: for large example, has smallest variance
<ul>
<li>see exercise 2</li>
</ul></li>
<li>delta method
<ul>
<li>the distribution is also equivariant, so interval</li>
</ul></li>
<li>approximately the Bayes estimator</li>
</ul></li>
<li>multiparameter method <strong>fix me</strong></li>
<li>parametric bootstrap method</li>
<li>numerical methods
<ul>
<li>Newton-Raphson</li>
<li>EM algorithm
<ul>
<li>hidden variable</li>
<li>maxiture of two normals</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="hypothesis-testing">hypothesis testing</h3>
<ul>
<li>we patition the parameter space into <code>Theata_0</code> and <code>Theata_1</code></li>
<li><code>H_0: theta in Theta_0</code> null hypothesis, and alternative hypothesis</li>
<li>rejection region <code>R</code>, <code>X</code> be a rv and <code>R &lt; Dom(X)</code>, we calculate a rv, and see if we reject the null hypothesis
<ul>
<li>it is normally form <code>R = {x: T(x) &gt; c}</code></li>
<li>test statistic, critical value</li>
</ul></li>
<li>type 1, 2 error</li>
<li>power function, size, level</li>
<li>simple hypothesis, composite hypothesis, two-sided test, one-sided test</li>
<li>Wald test <strong>see book</strong></li>
<li>p-value = <code>inf {a: T(X^n) in R_a}</code>, smallest level we can reject <code>H_0</code>
<ul>
<li>large p-value can occur in two reason: true or test has low power!</li>
</ul></li>
<li><strong>fix me</strong></li>
</ul>
<h3 id="bayesian-inference">Bayesian inference</h3>
<ul>
<li>Bayes theorem <code>f(theta|x) =   f(x|theta)f(theta) / integral f(x|theta) f(theta) d theta</code></li>
<li><code>f(theta|x^n) = L_n(theta)f(theta) / c_n</code></li>
<li>conjugate with model</li>
<li><strong>fix me</strong></li>
<li>what prior to use?
<ul>
<li>..</li>
</ul></li>
<li><strong>fix me</strong></li>
</ul>
<h3 id="dicision-theory">dicision theory</h3>
<p><em>how to? is it not just calculate the estimated error?</em></p>
<ul>
<li>how to choose estimator: decision theory</li>
<li>decision rule = estimator, action = values of estimator</li>
<li>loss function</li>
<li>risk <code>R(theta, theta-head)</code>
<ul>
<li>e.g. when you are using squared error… the risk is just mse!!!</li>
<li>maximum risk <code>R-bar(theta-head)</code></li>
<li>Bayes risk <code>r(f, theta-head)</code></li>
</ul></li>
<li>Bayes rule, minimax ruleok</li>
<li><strong>fix me</strong></li>
</ul>
<h2 id="models-and-methods">models and methods</h2>
<h3 id="linear-and-logistic-regression">linear and logistic regression</h3>
<ul>
<li>regression fuction <code>r(x) = b_0 + b_1 x</code></li>
<li>assuming <code>V(e_i|X = x) = sigma^2</code> do not depend on <code>x</code></li>
<li><code>Y_i = b_0 + b_1 X_i + e_i</code>, <code>E(e_i|X_i) = 0</code>
<ul>
<li><code>sigma</code> is also a paramter of the model!</li>
<li>fitted line <code>r-head(x) = b_0-head + b_1-head x</code></li>
<li>fitted values <code>Y_i-head = r-head(X_i)</code></li>
<li>residuals <code>e_i-head = Y_i - Y_i-head</code></li>
<li>residual sums of squares <code>RSS = Sum ( e_i-head ^2 )</code></li>
<li>least square estimates: minimize rss, calculate them using dervi!
<ul>
<li><code>b_1-head = Sum( (X_i - X_n-bar)(Y_i - Y_n-bar) ) / Sum( (X_n - X_n-bar)^2 )</code></li>
<li><code>b_0-head = Y_n-head - b_1-head X_n-bar</code></li>
<li><code>sigma^2-head = RSS / (n - 2)</code></li>
<li><code>E(b-head|X^n) = (b_0; b_1)</code> this is a constant function of <code>X^n</code></li>
<li><code>V(b-head|X^n) =</code> see the book, this means if you pick <code>x</code>s too close, you will have a very bad estimate</li>
<li><code>se-head(b_0|X^n)</code> and <code>se-head(b_1|X^n)</code></li>
<li>consistant</li>
<li>asymptotic normality</li>
<li>approximate <code>1-a</code> interval is <strong>see book</strong></li>
<li>Wald test</li>
</ul></li>
<li>under normal assumption, lse is mle</li>
<li>prediction interval <strong>fix me</strong></li>
<li>multiple regression
<ul>
<li><em>the model assumed when we know we are sampling from independent <code>x</code>s!!!!</em></li>
</ul></li>
<li>model selection, overfitting, underfitting
<ul>
<li>prediction risk, training error</li>
<li><code>C_p</code> statistic</li>
<li><strong>fix me</strong></li>
</ul></li>
</ul></li>
<li>logistic regression</li>
</ul>
<h3 id="multivariate-models">multivariate models</h3>
<h3 id="inference-about-independence">inference about independence</h3>
<h3 id="causal-inference">causal inference</h3>
<h3 id="directed-graphs">directed graphs</h3>
<h3 id="undirected-graphs">undirected graphs</h3>
<h3 id="log-linear">log-linear</h3>
<h3 id="nonparametric-curve-estimation">nonparametric curve estimation</h3>
<h3 id="smoothing-using-orthogonal-functions">smoothing using orthogonal functions</h3>
<h3 id="classification">classification</h3>
<ul>
<li>classification = pattern recognition
<ul>
<li>the estimation process is learning!</li>
</ul></li>
<li>input <code>X_i = (X_i1,..., X_id) in R^d</code> is a n number of d-dim input, find <code>h(R^d) -&gt;</code></li>
<li>true error rate <code>L(h) = P(h(X) != Y)</code></li>
<li>eer <code>L_n-head(h) = Sum(I(h(X_i) != Y_i)) / n</code></li>
<li>regression function <code>r(x) = P(Y = 1| X = x)</code>
<ul>
<li><em>it is a probability, because there might be other DOF in the model!!!</em></li>
</ul></li>
<li>Bayes classification rule <code>h* = 1 if r(x) &gt; 1/2</code></li>
<li>multi version <code>h(x) = argmax_k P(Y = k|X = x)</code>
<ul>
<li><code>P(Y = k|X = x) = f_k(x)pi_k / Sum( f_i(x)pi_i )</code></li>
<li><code>pi_i = P(Y = i)</code>, <code>f_i(x) = f(x|Y = i)</code></li>
</ul></li>
<li>ways?
<ul>
<li>empirical risk mini</li>
<li>regression</li>
<li>density estimation</li>
</ul></li>
<li>Gaussian classifiers
<ul>
<li>assume both are multivariate Gaussian <strong>see book</strong></li>
<li><code>h*(x) = argmax_k( -1/2*log|Pi_k| - 1/2*(x-mu_k)'Pi_k^-1(x-mu_k) + log pi_k )</code></li>
<li>sample esimates</li>
<li>qda</li>
<li>simplification when <code>Pi_1 = Pi_0</code> <strong>fix me</strong>, lda</li>
<li>Fisher lienar discrimination</li>
</ul></li>
<li>linear regression: model the <code>r(x)</code> using a linear function!!!
<ul>
<li>relationship logi stic and lda
<ul>
<li>parameters estimation!
<ul>
<li>lr: discriminative learning</li>
<li>lda: generative learning</li>
</ul></li>
</ul></li>
</ul></li>
<li>density estimation and naive Bayes</li>
</ul>
<h3 id="simulation-methods">simulation methods</h3>
<ul>
<li>Monte Carlo integration</li>
<li>Metropolis-Hastings algorithm</li>
<li>Gibbs sampling</li>
</ul>
<h2 id="references">references</h2>
<ul>
<li><span class="crosslink"><span id="cross-ref-stat-all"></span>ref-stat-all</span> <em>All of Statistics</em></li>
</ul>
    </div>
</div>
 
</div>
<script src="../js/jquery.toc.js"></script>
<script src="../js/toc.js"></script>




<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



<!--<script src="/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->

<script>
  MathJax.Hub.Queue(function() {
    $(document.body).scrollspy('refresh')
  })
</script>


  </div>

    <div class="footer">
      <div class="container">
        <span></span>
      </div>
    </div>

      <script src="../js/bootstrap.min.js"></script>
      <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50798592-1', 'snailya.org');
  ga('send', 'pageview');

</script>
    </body>
</html>
